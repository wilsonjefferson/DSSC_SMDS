theta_est <- mean(sim$theta)
library(LearnBayes)
y=c(1,0,0,1,0,0,0,0,0,1,0,0,1,0)
n <-  length(y)
heads <- sum(y == 1)
tails <- n - heads
alpha = 3
beta = 3
data<- list(N=n, y=heads, alpha=alpha, beta=beta)
fit <- stan(file="beta-binomial.stan", data = data, chains = 4, iter=2000,refresh=-1)
#extract the Stan output
sim <- extract(fit)
theta_est <- mean(sim$theta)
theta_var <- var(sim$theta)
c(theta_est, theta_var)
posterior <- as.matrix(fit)
#traceplot
traceplot(fit, pars ="theta")
plot_title <- ggtitle("Posterior distributions","with medians and 80% intervals")
mcmc_areas(posterior, pars = c("theta"), prob = 0.8) + plot_title
library(LearnBayes)
y=c(1,0,0,1,0,0,0,0,0,1,0,0,1,0)
n <-  length(y)
heads <- sum(y == 1)
tails <- n - heads
alpha = 3
beta = 3
data<- list(N=n, y=heads, alpha=alpha, beta=beta)
fit <- stan(file="beta-binomial.stan", data = data, chains = 4, iter=2000,refresh=-1)
#extract the Stan output
sim <- extract(fit)
theta_est <- mean(sim$theta)
theta_var <- var(sim$theta)
c(theta_est, theta_var)
posterior <- as.matrix(fit)
#traceplot
traceplot(fit, pars ="theta")
plot_title <- ggtitle("Posterior distributions","with medians and 80% intervals")
mcmc_areas(posterior, pars = c("theta"), prob = 0.8) + plot_title
median(sim$theta)
summary(sim$theta)
var(sim$theta)
?curve
?dbeta
?dbeta
curve(dbeta(x,alpha,beta))
curve(dbeta(x,alpha,beta), xlim = c(-1,2))
curve(dbeta(x,alpha,beta), xlim = c(-1,2),ylim = c(0,4))
curve(dbeta(x,alpha,beta), xlim = c(-1,2),ylim = c(0,2), lty =1, ylab = "Prior", xlab = expression(theta))
curve(dbeta(x,alpha+4,beta+10), xlim = c(-1,2),ylim = c(0,2), lty =1, ylab = "Prior", xlab = expression(theta))
curve(dbeta(x,alpha+4,beta+10), xlim = c(-1,2),ylim = c(0,5), lty =1, ylab = "Prior", xlab = expression(theta))
# Prior distribution
curve(dbeta(x, alpha, beta), ylab = "density", lty = 2, col = "red", xlim = c(-0.25,1.25), ylim=c(0,4))
# Stan posterior distribution
lines(density(sim$theta, adj=2), col="blue", lty=2, add=TRUE)
# True posterior distribution
curve(dbeta(x, alpha + N_1, beta + N_0), lty=1, col="black", add=TRUE)
y <- c(1,0,0,1,0,0,0,0,0,1,0,0,1,0)
n <- length(y)
N_1 <- sum(y) # Positive outcome estimate
N_0 <- n - N_1 # Negative outcome estimate
alpha <- 3
beta <- 3
data <- list(N=n, y=N_1, alpha=alpha, beta=beta)
fit <- stan(file="beta-binomial.stan", data = data, chains = 4, iter = 2000)
sim <- extract(fit)
posterior_biv <- as.matrix(fit)
plot_title <- ggtitle("Posterior distributions", "with medians and 80% intervals")
mcmc_areas(posterior_biv, pars = c("theta"), prob = 0.8) + plot_title
mcmc_intervals(posterior_biv, pars = c("theta"))
# Prior distribution
curve(dbeta(x, alpha, beta), ylab = "density", lty = 2, col = "red", xlim = c(-0.25,1.25), ylim=c(0,4))
# Stan posterior distribution
lines(density(sim$theta, adj=2), col="blue", lty=2, add=TRUE)
# True posterior distribution
curve(dbeta(x, alpha + N_1, beta + N_0), lty=1, col="black", add=TRUE)
legend(0.75, 3.5, c("Prior distribution", "Stan posterior", "True posterior"), c("red", "blue", "black"))
library(LearnBayes)
y=c(1,0,0,1,0,0,0,0,0,1,0,0,1,0)
n <-  length(y)
heads <- sum(y == 1)
tails <- n - heads
alpha = 3
beta = 3
data<- list(N=n, y=heads, alpha=alpha, beta=beta)
fit <- stan(file="beta-binomial.stan", data = data, chains = 4, iter=2000,refresh=-1)
#extract the Stan output
sim <- extract(fit)
summary(sim$theta)
var(sim$theta)
posterior <- as.matrix(fit)
#traceplot
traceplot(fit, pars ="theta")
plot_title <- ggtitle("Posterior distributions","with medians and 80% intervals")
mcmc_areas(posterior, pars = c("theta"), prob = 0.8) + plot_title
curve(sim$theta)
sim$theta
hist(sim$theta)
plot(sim$theta)
lines(sim$theta)
curve(dbeta(x,alpha,beta), xlim = c(-1,2),ylim = c(0,2), lty =1, ylab = "Prior", xlab = expression(theta))
curve(dbeta(x,alpha+4,beta+10), xlim = c(-1,2),ylim = c(0,5), lty =1, ylab = "True Posterior", xlab = expression(theta))
curve(dbeta(x,alpha,beta), xlim = c(-1,2),ylim = c(0,2), lty =1, ylab = "Prior", xlab = expression(theta))
curve(dbeta(x,alpha,beta), xlim = c(-1,2),ylim = c(0,2), lty =1, ylab = "Prior", xlab = expression(theta))
curve(dbeta(x,alpha+4,beta+10), xlim = c(-1,2),ylim = c(0,5), lty =1, ylab = "True Posterior", xlab = expression(theta))
curve(dbeta(x,alpha,beta), xlim = c(-1,2),ylim = c(0,2), lty =1, ylab = "Prior", xlab = expression(theta))
curve(dbeta(x,alpha+4,beta+10), xlim = c(-1,2),ylim = c(0,5), lty =1, ylab = "True Posterior", xlab = expression(theta))
curve(dbeta(x,alpha,beta),  lty =1, ylab = "Prior", xlab = expression(theta))
curve(dbeta(x,alpha+4,beta+10), lty =1, ylab = "True Posterior", xlab = expression(theta))
legend(c("Prior", "True Posterior"))
legend(c("Prior", "True Posterior"), c("red", "black"))
?legend
curve(dbeta(x,alpha,beta),  lty =1, ylab = "Density", xlab = expression(theta))
curve(dbeta(x,alpha+4,beta+10), lty =1, xlab = expression(theta))
#legend(c("Prior", "True Posterior"), c("red", "black"))
curve(dbeta(x,alpha,beta),  lty =1, ylab = "Density", xlab = expression(theta))
curve(dbeta(x,alpha+4,beta+10), lty =1, xlab = expression(theta))
legend(0.8,1.5,c("Prior", "True Posterior"), c("red", "black"))
curve(dbeta(x,alpha,beta), xlim=c(-0.25,1.25), lwd =2,ylab = "Density", xlab = expression(theta))
curve(dbeta(x,alpha+4,beta+10), lty =2, col="red", add=TRUE,lwd=2)
legend(0.8,1.5,c("Prior", "True Posterior"), c("red", "black"))
curve(dbeta(x,alpha,beta), xlim=c(-0.25,1.25), ylim= c(0,4) lwd =2,ylab = "Density", xlab = expression(theta))
curve(dbeta(x,alpha,beta), xlim=c(-0.25,1.25), ylim= c(0,4), lwd =2,ylab = "Density", xlab = expression(theta))
curve(dbeta(x,alpha+4,beta+10), lty =2, col="red", add=TRUE,lwd=2)
legend(0.8,1.5,c("Prior", "True Posterior"), c("red", "black"))
legend(0.8,2.5,c("Prior", "True Posterior"), c("red", "black"))
curve(dbeta(x,alpha,beta), xlim=c(-0.25,1.25), ylim= c(0,4), lwd =2,ylab = "Density", xlab = expression(theta))
curve(dbeta(x,alpha+4,beta+10), lty =2, col="red", add=TRUE,lwd=2)
legend(0.8,2.5,c("Prior", "True Posterior"), c("red", "black"))
curve(dbeta(x,alpha,beta), xlim=c(-0.25,1.25), ylim= c(0,4), lwd =2,ylab = "Density", xlab = expression(theta))
curve(dbeta(x,alpha+4,beta+10), lty =2, col="red", add=TRUE,lwd=2)
legend(0.8,3.5,c("Prior", "True Posterior"), c("red", "black"))
curve(dbeta(x,alpha,beta), xlim=c(-0.25,1.25), ylim= c(0,4), lwd =2,ylab = "Density", xlab = expression(theta))
curve(dbeta(x,alpha+4,beta+10), lty =2, col="red", add=TRUE,lwd=2)
legend(1.0,3.5,c("Prior", "True Posterior"), c("red", "black"))
curve(dbeta(x,alpha,beta), xlim=c(-0.25,1.25), ylim= c(0,4), lwd =2,ylab = "Density", xlab = expression(theta))
curve(dbeta(x,alpha+4,beta+10), lty =2, col="red", add=TRUE,lwd=2)
legend(0.8,3.5,c("Prior", "True Posterior"), c("red", "black"))
curve(dbeta(x,alpha,beta), xlim=c(-0.25,1.25), ylim= c(0,4), lwd =2,ylab = "Density", xlab = expression(theta))
curve(dbeta(x,alpha+4,beta+10), lty =2, col="red", add=TRUE,lwd=2)
lines(density(sim$theta),lty=2, col="blue")
legend(0.8,3.5,c("Prior", "True Posterior"), c("red", "black"))
curve(dbeta(x,alpha,beta), xlim=c(-0.25,1.25), ylim= c(0,4), lwd =2,ylab = "Density", xlab = expression(theta))
curve(dbeta(x,alpha+4,beta+10), lty =2, col="red", add=TRUE,lwd=2)
lines(density(sim$theta),lty=2, col="blue", add=TRUE)
legend(0.8,3.5,c("Prior", "True Posterior"), c("red", "black"))
curve(dbeta(x,alpha,beta), xlim=c(-0.25,1.25), ylim= c(0,4), lwd =2,ylab = "Density", xlab = expression(theta))
curve(dbeta(x,alpha+10,beta+4), lty =2, col="red", add=TRUE,lwd=2)
lines(density(sim$theta),lty=2, col="blue", add=TRUE)
legend(0.8,3.5,c("Prior", "True Posterior"), c("red", "black"))
curve(dbeta(x,alpha,beta), xlim=c(-0.25,1.25), ylim= c(0,4), lwd =2,ylab = "Density", xlab = expression(theta))
curve(dbeta(x,alpha+4,beta+10), lty =2, col="red", add=TRUE,lwd=2)
lines(density(sim$theta),lty=2, col="blue", add=TRUE)
legend(0.8,3.5,c("Prior", "True Posterior","Stan Posterior"), c("black", "red","blue"))
?glm
# We will make use of this package (install it if not present)
library(forecast)
# We will make use of this package (install it if not present)
library(forecast)
version
# In this tutorial, we will see how to model time series in R and, then,
# how we can make predictions of future time series data.
# We will make use of this package (install it if not present)
library(forecast)
# Load a simple time series database, already built-in in R
# The classic Box & Jenkins airline data.
# It records monthly totals of international airline passengers, from 1949 to 1960.
data(AirPassengers)
class(AirPassengers)
frequency(AirPassengers)
# So, this dataset in R is stored as a ts object of frequency 12
# Let us now see what we have loaded
AirPassengers
# it is a matrix (row=years, columns=months)
start(AirPassengers)
# data begins on January 1949
end(AirPassengers)
# data ends on December 1960
length(AirPassengers)
# 144 values, so 1 value per month, since we are considering 12 years
summary(AirPassengers)
# There seems to be quite some variability in the data
# Let us now plot the data
plot(AirPassengers)
# As we can see, the time series has a clear upward trend
# Moreover, it shows seasonality effects: winter months have typically less passengers than summer ones
# In addition, also observe that variability is increasing with time
# So, this time series is clearly not stationary
# The trend can also be highlighted by fitting a straigth line into our plot
abline(reg=lm(AirPassengers~time(AirPassengers)), col='red')
# We can get confirmation about our thoughts about winter and summer months
# passenger traffic aggregating the data by month, and making some boxplots.
# Indeed, summer months have more passenger traffic
boxplot(AirPassengers~cycle(AirPassengers))
# As you can see here, for each data point in the time series,
# function cycle tells you the position of that particular data point in that cycle
print(cycle(AirPassengers))
# Let us try to decompose the series into trend, seasonality and residual
# As we have seen, since in this case the seasonality effects depend on time,
# we have to rely on a multiplicative model to do that
decomposeAirPassengers <- decompose(AirPassengers, "multiplicative")
autoplot(decomposeAirPassengers)
# Now, let us make some forecasts!
# The first step is making the time series stationary.
# Differencing is a classical operation by which we can do that.
# Differencing can help stabilize the mean of the time series by removing changes in the level
# of a time series, and so eliminating (or reducing) trend and seasonality effects.
# Differencing is performed by subtracting the previous observation from the current observation.
differenced <- diff(AirPassengers, differences= 1)
plot(differenced, type="l", main="Differenced and Stationary")
# Uh oh, the result is clearly not stationary... what happened here?
# Well, in our case, since the time series appears to be seasonal, a better approach is to subtract
# from each value of the time series the value that was observed in the same season one year earlier.
# Let's try that: apply seasonal differencing
AirPassengers_seasdiff <- diff(AirPassengers, lag=frequency(AirPassengers), differences=1)
plot(AirPassengers_seasdiff, type="l", main="Seasonally Differenced")
# Uhm, better... but the time series is still not stationary, there is a clear trend here...
# Let us now apply classical differencing on the seasonally differenced time series
stationaryTS <- diff(AirPassengers_seasdiff, differences= 1)
plot(stationaryTS, type="l", main="Differenced and Stationary")
# Now, this is stationary!
# In order to train our forecasting models and evaluate them,
# we now subset train and test data using the window() function
train_data = window(stationaryTS, end = c(1957,12))
test_data = window(stationaryTS, start = c(1958,1))
# First of all, let us try the naive method, that just copies the last value
# of the time series (there also exists snaive, which copies the corresponding seasonal
# value from the previous season, but we do not need it here, having a stationary time series)
naive_method_forecast = naive(train_data, h=36, level = c(80, 95))
plot(naive_method_forecast)
lines(test_data, col='red')
summary(naive_method_forecast)
# Results are not exceptional (around 15 training set RMSE), however naive is useful to establish a baseline result
# In the plot, we can also see the 80% and 95% confidence intervals, respecively in darker and lighter blue color
# Now, we may use a more complex technique, Simple Exponential Smoothing
# Simple exponential smoothing assumes that the time series data has only a level and some error (also known as remainder)
# but no trend or seasonality, which is the case of our stationary time series data.
# In exponential smoothing, all past observations are part of the calculation for the forecasted value.
# The smoothing parameter ALPHA determines the distribution of weights of past observations and, with that,
# how heavily a given time period is factored into the forecasted value.
# If the smoothing parameter is close to 1, recent observations carry more weight;
# otherwise, if the smoothing parameter is closer to 0, weights of older and recent observations are more balanced.
ses_method_forecast = ses(train_data, h=36, level = c(80, 95))
plot(ses_method_forecast)
lines(test_data, col='red')
summary(ses_method_forecast)
# This is a little bit better, with a training set RMSE of around 10
# As you can see, SES just projects a flatlined estimate into the future.
# This is the reason why it should not be used on data with a trend or seasonal component
# However, there are some variants of Exponential Smoothing that can handle that (see bottom of the tutorial)...
# Finally, we apply ARIMA
# ARIMA stands for Auto Regressive Integrated Moving Average, which indicates that an ARIMA model has three components, i.e.:
# - Auto-Regressive: past time series points may impact its current and future values. Thus, ARIMA uses p past observations
#                    to forecast current and future values. Auto-regression is employed, that is, the process of applying
#                    regression on a variable considering past values of itself. In other words, the current value of the series
#                    is determined as a linear combination of p past values (which may also be previous forecasts).
# - Integrated: the forecasting method cannot be applied to non-stationary time series. Thus, differencing is applied to
#               reduce trend and seasonality effects.
# - Moving Average: also past noise in the series might influence its current and future values. For instance, a "shock" in a
#                   stock market time series will persist in a smaller extent in the next days as well. Thus, rather than
#                   using the past values of the forecast, the moving average model uses past forecast errors in a
#                   regression-like fashion. Specifically, the result is a weighted moving average over past forecast errors.
#
# The R function to call ARIMA is (unsurprisingly) named "arima", and features three parameters:
# p: number of lag values to consider in the auto-regressive part  (e.g., p=3 means we will use the 3 previous values of
#    our time series in the autoregressive portion of the calculation)
# d: number of differencing transformations applied to the time series to make it stationary (from previous experiments,
#    we already know we need 1 seasonal differencing and 1 "plain" differencing)
# q: the size of the moving average window
# The parameter "order" encodes the three previously mentioned components (p, d, q)
# For seasonal ARIMA, parameter "seasonal" encodes the three components (p, d, q) for the seasonal part, plus the length
# of the period (12 months in our case)
#
# The interested reader may find further details regarding ARIMA for instance here:
# http://people.cs.pitt.edu/~milos/courses/cs3750/lectures/class16.pdf
# We start from the original time series data, since the differencing is applied within ARIMA
train_data_arima = window(AirPassengers, end = c(1957,12))
test_data_arima = window(AirPassengers, start = c(1958,1))
arima_model <- arima(train_data_arima, order=c(2, 1, 0), seasonal=list(order=c(2, 1, 0), period = 12))
arima_forecast = forecast(arima_model, n=36, level = c(80, 95))
plot(arima_forecast)
lines(test_data_arima, col='red')
summary(arima_model)
# It does much better than previous - quite naive - methods!
# Ok, now, for instance, you can refer to this webpage and try other, more complex, variants of Exponential Smoothing!
# http://uc-r.github.io/ts_exp_smoothing
# Also, you may try to apply the naive and Simple Eponential Smoothing methods on the original, non-stationary time series
# Or, just play around with ARIMA parameters: can yoyu get a better fit?
score <- read.table("student_score.txt",header = TRUE)
# Parameter of interest PSI
# The parameter of interest is the eigenratio statistic for the correlation
# matrix of student_score: PSI = largest eigenvalue/sum eigenvalues:
psi_fun <- function(data) {
eig <- eigen(cor(data))$values
return(max(eig) / sum(eig))
}
# Observed value
psi_obs <- psi_fun(score)
# Compute confidence intervals
n <- length(score[,1]); B <- 10^4
s_aux <- rep(0,n)
s_vect <- rep(0, B)
SE_jack <- rep(0, B)
for(i in 1:B){
ind <- sample(1:n, n, replace = TRUE)
s_vect[i] <- psi_fun(score[ind,])
for(j in 1:n) s_aux[j] <- psi_fun(score[ind,][-j,]) # Sample without j-th obs
SE_jack[i] <- sqrt(((n - 1)/n) * sum((s_aux - mean(s_aux))^2))
}
SE_boot <- sd(s_vect)
psi_obs + c(-1, 1) * 1.96 * SE_boot
# Studentized bootstrap confidence interval
z<-(s_vect - psi_obs)/SE_jack
studentized_ci <- psi_obs - quantile(z, prob=c(0.975, 0.025))*SE_boot
studentized_ci
# Percentile method
perc_ci <- quantile(s_vect, prob=c(0.025, 0.975))
attr(perc_ci, "names") <- NULL
perc_ci
# Basic method
basic_ci <- 2 * psi_obs - quantile(s_vect, prob=c(0.975, 0.025))
attr(basic_ci, "names") <- NULL
basic_ci
Intervals<-c("Basic CI", "Perc CI", "Stud CI")
data <- data.frame(basic_ci, perc_ci, studentized_ci)
data <- data.frame(t(data), Intervals)
par(mfrow=c(1,2))
ggplot(data, aes(x=Intervals, color = Intervals)) +
ggtitle("Plot of Confidence Intervals") +
xlab("Confidence Intervals") + ylab(" ") +
geom_errorbar( aes(x=data$Intervals, ymin=data$X2.5., ymax=data$X97.5.), width=0.4, alpha=0.9, size=1.3)+
geom_abline(slope = 0, intercept = psi_obs, colour = "red", linetype = 2, size=1)+
geom_text(aes(x = 2.5, y = .705, label = "PSI-observed"), colour = "red")
data <- data.frame(s_vect)
ggplot(data, aes(x=s_vect, y=..ncount..))+
xlab("s_vect")+ ylab("Density") +
geom_histogram(binwidth = .005, fill="light grey") +
geom_segment(aes(x = psi_obs , y = 1, xend = psi_obs, yend = 0), col=2) +
geom_text(aes(x = .75, y = .9, label = "PSI-observed"), colour = "red") +
geom_segment(aes(x = basic_ci[1] , y = 1, xend = basic_ci[1], yend = 0), col=3) +
geom_segment(aes(x = basic_ci[2] , y = 1, xend = basic_ci[2], yend = 0), col=3)  +
geom_text(aes(x = .4, y = .75, label = "Basic CI"), colour = 3) +
geom_segment(aes(x = perc_ci[1] , y = 1, xend = perc_ci[1], yend = 0), col=4) +
geom_segment(aes(x = perc_ci[2] , y = 1, xend = perc_ci[2], yend = 0), col=4) +
geom_text(aes(x = .4, y = .50, label = "Percentile CI"), colour = 4) +
geom_segment(aes(x = studentized_ci[1] , y = 1, xend = studentized_ci[1], yend = 0), col=5) +
geom_segment(aes(x = studentized_ci[2] , y = 1, xend = studentized_ci[2], yend = 0), col=5) +
geom_text(aes(x = .4, y = .25, label = "Studentized CI"), colour = 5)
library(MASS)
library(knitr)
local({
hook_plot = knit_hooks$get('plot')
knit_hooks$set(plot = function(x, options) {
paste0('\n\n----\n\n', hook_plot(x, options))
})
})
#loading data
score <- read.table("files/student_score.txt", header = TRUE)
# function fo compute the parameter of interest --> eignratio
psi_fun <- function(data) {eig <- eigen(cor(data))$values
return(max(eig) / sum(eig))}
psi_obs <- psi_fun(score)
#psi_obs
# estimate the bootstrap-based standard error, SE_boot:
# and estimate the Standard error from each bootstrap sample using jackknife:
n <- nrow(score)
B <- 10^4
s_vect <- rep(0,B)
SE_jack <- rep(0,B)
s_tmp <- rep(0, n)
for(i in 1:B)
{
ind <- sample(1:n, n, replace = TRUE)
s_vect[i] <- psi_fun(score[ind,])
for(j in 1:n) {s_tmp[j] <- psi_fun(score[ind,][-j,])}
SE_jack[i] <- sqrt(((n - 1)/n) * sum((s_tmp - mean(s_tmp))^2))
}
#loading data
score <- read.table("files/student_score.txt", header = TRUE)
# function fo compute the parameter of interest --> eignratio
psi_fun <- function(data) {eig <- eigen(cor(data))$values
return(max(eig) / sum(eig))}
psi_obs <- psi_fun(score)
#psi_obs
# estimate the bootstrap-based standard error, SE_boot:
# and estimate the Standard error from each bootstrap sample using jackknife:
n <- nrow(score)
B <- 10^4
s_vect <- rep(0,B)
SE_jack <- rep(0,B)
s_tmp <- rep(0, n)
for(i in 1:B)
{
ind <- sample(1:n, n, replace = TRUE)
s_vect[i] <- psi_fun(score[ind,])
for(j in 1:n) {s_tmp[j] <- psi_fun(score[ind,][-j,])}
SE_jack[i] <- sqrt(((n - 1)/n) * sum((s_tmp - mean(s_tmp))^2))
}
SE_boot <- sd(s_vect)
wald_ci <- psi_obs + c(-1, 1) * 1.96 * SE_boot
print("Wald-type confidence interval :")
wald_ci
hist.scott(s_vect, main = "Wald-type CI")
abline(v = psi_obs, col = 2)
abline(v = wald_ci, col = "blue")
#percentile method :
perc_ci <- quantile(s_vect, prob=c(0.025, 0.975))
attr(perc_ci, "names") <- NULL
perc_ci
hist.scott(s_vect, main = "Percentile method")
abline(v = psi_obs, col = 2)
abline(v = perc_ci, col = "blue")
#studentized Method
z_vect <- (s_vect - psi_obs)/SE_jack
#studentized bootstrap confidence interval :
stud_ci <- psi_obs - SE_boot * quantile(z_vect, prob=c(0.975, 0.025))
attr(stud_ci, "names") <- NULL
print("Studentized bootstrap confidence interval :")
stud_ci
hist.scott(s_vect, main = "Studentized method")
abline(v = psi_obs, col = 2)
abline(v = stud_ci, col = "blue")
#Sample.
y <- c(155.9, 200.2, 143.8, 150.1,152.1, 142.2, 147, 146, 146,
170.3, 148, 140, 118, 144, 97)
n <- length(y)
log_lik_weibull <- function( data, param){
-sum(dweibull(data, shape = param[1], scale = param[2], log = TRUE))
}
theta <- function(omega) exp(omega)
omega <- function(theta) log(theta)
log_lik_weibull_rep <- function(data, param) log_lik_weibull(data, theta(param))
weib.y.nlm<-nlm(log_lik_weibull_rep,c(0,0),hessian=T,data=y)
weib.y.nlm
diag(solve(weib.y.nlm$hessian))
?optimHess()
theta <- optimHess(theta(weib.y.nlm$estimate),log_lik_weibull,data=y)
theta_var <- optimHess(theta(weib.y.nlm$estimate),log_lik_weibull,data=y)
theta <- function(omega) exp(omega)
#Sample.
y <- c(155.9, 200.2, 143.8, 150.1,152.1, 142.2, 147, 146, 146,
170.3, 148, 140, 118, 144, 97)
n <- length(y)
log_lik_weibull <- function( data, param){
-sum(dweibull(data, shape = param[1], scale = param[2], log = TRUE))
}
omega <- function(theta) log(theta)
theta <- function(omega) exp(omega)
log_lik_weibull_rep <- function(data, param) log_lik_weibull(data, theta(param))
weib.y.nlm<-nlm(log_lik_weibull_rep,c(0,0),hessian=T,data=y)
weib.y.nlm
diag(solve(weib.y.nlm$hessian))
?optimHess()
theta_var <- optimHess(theta(weib.y.nlm$estimate),log_lik_weibull,data=y)
theta_var
diag(solve(theta_var))
theta_var <- optimHess(theta(weib.y.nlm$estimate),log_lik_weibull)
theta_var <- optimHess(theta(weib.y.nlm$estimate),log_lik_weibull,data=y)
library(LearnBayes)
y=c(1,0,0,1,0,0,0,0,0,1,0,0,1,0)
n <-  length(y)
heads <- sum(y == 1)
tails <- n - heads
alpha = 3
beta = 3
data<- list(N=n, y=heads, alpha=alpha, beta=beta)
fit <- stan(file="files/beta-binomial.stan", data = data, chains = 4, iter=2000,refresh=-1)
#extract the Stan output
sim <- extract(fit)
summary(sim$theta)
var(sim$theta)
posterior <- as.matrix(fit)
#traceplot
traceplot(fit, pars ="theta")
plot_title <- ggtitle("Posterior distributions","with medians and 80% intervals")
mcmc_areas(posterior, pars = c("theta"), prob = 0.8) + plot_title
y=c(1,0,0,1,0,0,0,0,0,1,0,0,1,0)
n <-  length(y)
heads <- sum(y == 1)
tails <- n - heads
alpha = 3
beta = 3
data<- list(N=n, y=heads, alpha=alpha, beta=beta)
fit <- stan(file="files/beta-binomial.stan", data = data, chains = 4, iter=2000,refresh=-1)
#extract the Stan output
sim <- extract(fit)
y=c(1,0,0,1,0,0,0,0,0,1,0,0,1,0)
n <-  length(y)
heads <- sum(y == 1)
tails <- n - heads
alpha = 3
beta = 3
data<- list(N=n, y=heads, alpha=alpha, beta=beta)
fit <- stan(file="files/beta-binomial.stan", data = data, chains = 4, iter=2000,refresh=-1)
library(LearnBayes)
y=c(1,0,0,1,0,0,0,0,0,1,0,0,1,0)
n <-  length(y)
heads <- sum(y == 1)
tails <- n - heads
alpha = 3
beta = 3
data<- list(N=n, y=heads, alpha=alpha, beta=beta)
fit <- stan(file="beta-binomial.stan", data = data, chains = 4, iter=2000,refresh=-1)
#extract the Stan output
sim <- extract(fit)
summary(sim$theta)
var(sim$theta)
posterior <- as.matrix(fit)
#traceplot
traceplot(fit, pars ="theta")
plot_title <- ggtitle("Posterior distributions","with medians and 80% intervals")
mcmc_areas(posterior, pars = c("theta"), prob = 0.8) + plot_title
