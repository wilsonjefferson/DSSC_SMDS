---
title: "Homework 3"
author: "Group B: Abdalghani, Demirbilek, Morichetti, Zambon"
date: "Spring 2020"
output:
  html_document:
    toc: no
header-includes:
- \usepackage{color}
- \definecolor{Purple}{HTML}{911146}
- \definecolor{Orange}{HTML}{CF4A30}
- \setbeamercolor{alerted text}{fg=Orange}
- \setbeamercolor{frametitle}{bg=Purple}
institute: University of Udine & University of Trieste
graphics: yes
fontsize: 10pt
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center', warning=FALSE, message=FALSE, fig.asp=0.625, dev='png', global.par = TRUE, dev.args=list(pointsize=10), fig.path = 'figs/', fig.height = 10, fig.width = 10)
```

```{r setup, include=FALSE}
library(MASS)
library(knitr)
local({
  hook_plot = knit_hooks$get('plot')
  knit_hooks$set(plot = function(x, options) {
    paste0('\n\n----\n\n', hook_plot(x, options))
  })
})
```

# {.tabset}

## LEC {.tabset}

### Exercise 1

**Compute the bootstrap-based confidence interval for the $score$ dataset using the studentized method.**

**Solution:**

```{r basic 1, echo=TRUE}
#loading data 
score <- read.table("files/student_score.txt", header = TRUE)

# function fo compute the parameter of interest --> eignratio 
psi_fun <- function(data) {eig <- eigen(cor(data))$values
                           return(max(eig) / sum(eig))}
psi_obs <- psi_fun(score)
#psi_obs

# estimate the bootstrap-based standard error, SE_boot:
# and estimate the Standard error from each bootstrap sample using jackknife: 
n <- nrow(score) 
B <- 10^4
s_vect <- rep(0,B)
SE_jack <- rep(0,B)
s_tmp <- rep(0, n)


for(i in 1:B) 
{
  ind <- sample(1:n, n, replace = TRUE)
  s_vect[i] <- psi_fun(score[ind,])
  for(j in 1:n) {s_tmp[j] <- psi_fun(score[ind,][-j,])}
  SE_jack[i] <- sqrt(((n - 1)/n) * sum((s_tmp - mean(s_tmp))^2))
  
}

SE_boot <- sd(s_vect)
wald_ci <- psi_obs + c(-1, 1) * 1.96 * SE_boot 
print("Wald-type confidence interval :")
wald_ci

#percentile method : 
perc_ci <- quantile(s_vect, prob=c(0.025, 0.975))
attr(perc_ci, "names") <- NULL
perc_ci

#studentized Method
z_vect <- (s_vect - psi_obs)/SE_jack
#studentized bootstrap confidence interval :
stud_ci <- psi_obs - SE_boot * quantile(z_vect, prob=c(0.975, 0.025))
attr(stud_ci, "names") <- NULL 
print("Studentized bootstrap confidence interval :")
stud_ci


hist.scott(s_vect, main = "Bootstrap-based Confidence intervasl methods")
abline(v = psi_obs , col = "red")
abline(v = wald_ci , col = "blue")
abline(v = perc_ci , col = "green")
abline(v = stud_ci , col = "orange")
legend("topleft", legend = c("psi_obs", "wald_ci", "perc_ci", "stud_ci"), col = c("red", "blue", "green", "orange"),lwd=c(2,2,2,2))
```

Comparing the Studentized bootstrap confidence interval and the Wald-type interval, and taking the point estimate as a reference, as given in the lecture the percentile confidence interval is wider on the left side and shorter on the right side. However, the studentized bootstrap confidence interval is wider on both sides (left and right).


### Exercise 2

**Compute bootstrap-based confidence intervals for the $score$ dataset using the $boot$ package.**

**Solution:**

```{r basic 3, echo=TRUE}
library(boot)

psi_fun_boot = function(x, indices){
  data <- x[indices,]
  eig <- eigen(cor(data))$values
  return(max(eig) / sum(eig))
}

#bootstrap variances needed for studentized intervals
psi_fun_boot_var = function(x, indices){
  data <- x[indices,]
  b <- boot(score, psi_fun_boot, 100)
  eig <- eigen(cor(data))$values
  return(c(max(eig) / sum(eig), var(b$t)))
}

score <- read.table("files/student_score.txt", header = TRUE)
boot_results <- boot(score, psi_fun_boot, 1000)
boot.ci(boot.out = boot_results, , type = c("perc","basic"))

boot_var_results <- boot(score, psi_fun_boot_var, 1000)
boot.ci(boot.out = boot_var_results, type = "stud")

```


## Laboratory {.tabset} 

### Exercise 1

**Use nlm to compute the variance for the estimator $\hat{w}=(log(\hat{γ}),log(\hat{β}))$ and `optimHess` for the variance of $\hat{θ}=(\hat{γ},\hat{β})$.**

**Solution:**

To compute the variance of an estimator we need to find first the hessian (that is the observed information matrix). Then we use solve to compute inverse of that matrix, that is the covariance matrix. 

```{r Ex 1.1, message = FALSE, warning = FALSE}

y <- c(155.9, 200.2, 143.8, 150.1,152.1, 142.2, 147, 146, 146,
       170.3, 148, 140, 118, 144, 97)

n <- length(y)

# log-likelihood function
log_lik_weibull <- function(data, param){
  -sum(dweibull(data, shape = param[1], scale = param[2], log = TRUE))
}

omega <- function(theta) log(theta)
theta <- function(omega) exp(omega)

# Variance of the w_hat estimator
# Since the parameters are already reparametrized in log scale
# we can use function log_lik_weibull_rep to find the omega estimator

log_lik_weibull_rep <- function(data, param) log_lik_weibull(data, theta(param))
weib.y.nlm <- nlm(log_lik_weibull_rep,c(0,0),hessian=T,data=y)
weib.y.nlm

cat("Variance of w:\n",diag(solve(weib.y.nlm$hessian)),"\n")

# Variance of the thetahat estimator using optimHess
opthess_output <- optimHess(theta(weib.y.nlm$estimate), log_lik_weibull, data=y)
cat("Variance of θ:\n",diag(solve(opthess_output)))

```

Above we have the variance results for $\hat{w}=(log(\hat{γ}),log(\hat{β}))$ and $\hat{θ}=(\hat{γ},\hat{β})$. First value is for the variance of parameter $\gamma$ and second value is the parameter $\beta$.


### Exercise 2

**The Wald confidence interval with level $1 - \alpha$ is defined as:**

$$
\hat{\gamma} +-z_{1-\alpha/2} j_p(\hat{\gamma})^{-1/2}
$$

**Compute the Wald confidence interval of level 0.95 and plot the results.**

**Solution:**

```{r Ex 2.2, warning = FALSE, message = FALSE, echo = TRUE}

y <- c(155.9, 200.2, 143.8, 150.1,152.1, 142.2, 147, 146, 146,
       170.3, 148, 140, 118, 144, 97)

n <- length(y)

# log-likelihood function
log_lik_weibull <- function(data, param){
  -sum(dweibull(data, shape = param[1], scale = param[2], log = TRUE))
}

log_lik_weibull_profile  <- function(data, gamma){
  beta.gamma <- mean(data^gamma)^(1/gamma)
 log_lik_weibull( data, c(gamma, beta.gamma) )
}

log_lik_weibull_profile_v <-Vectorize(log_lik_weibull_profile, 'gamma')

weib.y.mle <- optim(c(1,1),fn=log_lik_weibull_profile,hessian=T,
 method='L-BFGS-B',lower=rep(1e-7,2),
 upper=rep(Inf,2),data=y)

# standard error from heissen matrix
weib.y.se<-sqrt(diag(solve(weib.y.mle$hessian)))

conf.level <- 0.95
wald_level <- 1.96*sqrt(weib.y.se[1])

lrt.ci1 <- c(uniroot(function(x) -log_lik_weibull_profile_v(y, x) + weib.y.mle$value + wald_level, c(1e-7, weib.y.mle$par[1]))$root,
             uniroot(function(x) -log_lik_weibull_profile_v(y, x) + weib.y.mle$value + wald_level, c(weib.y.mle$par[1], 15))$root)

plot(function(x) -log_lik_weibull_profile_v(data=y, x) + weib.y.mle$value, 
     from = 0.1, to = 15,
     xlab = expression(gamma),
     ylab = 'profile relative log likelihood',
     ylim = c(-8,0))


lrt.ci1 <- c(uniroot(function(x) -log_lik_weibull_profile_v(y, x) + weib.y.mle$value + wald_level, c(1e-7, weib.y.mle$par[1]))$root,
             uniroot(function(x) -log_lik_weibull_profile_v(y, x) + weib.y.mle$value + wald_level, c(weib.y.mle$par[1], 15))$root)


abline(h = -log_lik_weibull_profile_v(y, lrt.ci1[1]) + weib.y.mle$value,lty = 'dashed', col=2)

segments(lrt.ci1[1],-log_lik_weibull_profile_v(y, lrt.ci1[1]) + weib.y.mle$value, 
         lrt.ci1[1], -log_lik_weibull_profile_v(y, lrt.ci1[1]), 
         col="red", lty = 2)

segments(lrt.ci1[2], -log_lik_weibull_profile_v(y, lrt.ci1[2]) + weib.y.mle$value, 
         lrt.ci1[2], -log_lik_weibull_profile_v(y, lrt.ci1[2]), 
         col="red", lty = 2)

points(lrt.ci1[1], -log_lik_weibull_profile_v(y, lrt.ci1[1]) + weib.y.mle$value, pch = 16, col = 2, cex = 1.5)
points(lrt.ci1[2], -log_lik_weibull_profile_v(y, lrt.ci1[2]) + weib.y.mle$value, pch = 16, col = 2, cex = 1.5)

segments(lrt.ci1[1], -8.1, 
         lrt.ci1[2], -8.1,
         col="red", lty = 1, lwd = 2)

text(7, -7.5, "95% Wald CI", col = 2)

```


### Exercise 3

**Repeat the steps of the previous exercise —write the profile log-likelihood, plot it and find the deviance confidence intervals— considering this time $\gamma$ as a *nuisance parameter* and $\beta$ as the *parameter of interest*.**


**Solution:**

We need to find the profile log likelihood:

\begin{equation*} l_P(\beta) = \max_{\gamma} l(\beta, \gamma; y) = l(\beta, \hat{\gamma}_{\beta}; y), \end{equation*}

where this time $\hat{\gamma}_{\beta}$ is the constrained MLE for $\gamma$, with $\beta$ fixed:

\begin{equation*} \hat{\gamma}_{\beta} = \big( \sum_{i=1}^n y_i^{\beta}/n\big)^{1/\beta}. \end{equation*}

It is not possible to express $\gamma$ as function of $\beta$ in a closed form so we used belove function which is the partial derivate of log likelihood function. 

\begin{equation*} \frac{n}{\gamma} - n\log(\beta) + \sum_{i=1}^n\log(y_i) - \sum_{i=1}^n \Big(\frac{y_i}{\beta}\Big)^{\gamma}\log\Big(\frac{y_i}{\beta}\Big) = 0 \end{equation*}


```{r Ex 3.1, message = FALSE, warning = FALSE}

y<-c(155.9, 200.2, 143.8, 150.1, 152.1, 142.2, 147, 146, 146, 
     170.3, 148, 118, 144, 97)

n <-  length(y)

log_lik_weibull <- function( data, param){
  -sum(dweibull(data, shape = param[1], scale = param[2], log = TRUE))
}

weib.y.mle<-optim(c(1,1),fn=log_lik_weibull,hessian=T,
                  method='L-BFGS-B',lower=rep(1e-7,2),
                  upper=rep(Inf,2),data=y)

gamma <- seq(0.1, 15, length=100)
beta <- seq(100,200, length=100)

#weib.y.mle$par

log_lik_weibull_profile  <- function(data, beta) {
  gamma.beta <- uniroot(function(x) n/x - n * log(beta) + sum(log(data)) - sum((data/beta)^x * log(data/beta)), c(1e-5,15))$root
  log_lik_weibull(data, c(gamma.beta, beta))
}

log_lik_weibull_profile_v <-Vectorize(log_lik_weibull_profile, 'beta'  )

plot(function(x) -log_lik_weibull_profile_v(data = y, x) + weib.y.mle$value, 
    from = 120, to = 200,
    xlab = expression(beta),
    ylab = 'profile relative log likelihood',
    ylim = c(-10,0))

conf.level <- 0.95
abline(h = -qchisq(conf.level,1)/2, lty = 'dashed', col = 2)


lrt.ci1 <- uniroot(function(x) -log_lik_weibull_profile_v(y, x) + weib.y.mle$value + qchisq(conf.level,1)/2,
                   c(1e-7, weib.y.mle$par[2]))$root

lrt.ci1 <- c(lrt.ci1, uniroot(function(x) -log_lik_weibull_profile_v(y,x) + weib.y.mle$value + qchisq(conf.level,1)/2,
                              c(weib.y.mle$par[2], 200))$root)

segments(lrt.ci1[1],-qchisq(conf.level,1)/2, lrt.ci1[1],
         -log_lik_weibull_profile_v(y, lrt.ci1[1]), col="red", lty = 2)

segments(lrt.ci1[2],-qchisq(conf.level,1)/2, lrt.ci1[2],
         -log_lik_weibull_profile_v(y, lrt.ci1[2]), col="red", lty = 2)

points(lrt.ci1[1], -qchisq(0.95,1)/2, pch = 16, col = 2, cex = 1.5)
points(lrt.ci1[2], -qchisq(0.95,1)/2, pch = 16, col = 2, cex = 1.5)

segments( lrt.ci1[1], -8.1, 
          lrt.ci1[2], 
          -8.1, col="red", lty = 1, lwd = 2)

text(155, -7.5, "95% Deviance CI", col = 2)

cat("Confidence interval is:",lrt.ci1[1],lrt.ci1[2])

```
Above plot shows the 95% confidence interval for parameter $\beta$.

### Exercise 5 

**In $sim$ in the code above, you find the MCMC output which allows to approximate the posterior distribution of our parameter of interest with $S$ draws of $\theta$. Please, produce an histogram for these random draws $\theta(1),…,\theta(S)$, compute the empirical quantiles, and overlap the true posterior distribution.**

**Solution:**

```{r basic = 7, echo=TRUE}
#true mean
theta_sample <- 2
#likelihood variance
sigma2 <- 2
#sample size
n <- 10
#prior mean
mu <- 7
#prior variance
tau2 <- 2

#generate some data
set.seed(123)
y <- rnorm(n,theta_sample, sqrt(sigma2))

#posterior mean
mu_star <- ((1/tau2)*mu+(n/sigma2)*mean(y))/( (1/tau2)+(n/sigma2))
#posterior standard deviation
sd_star <- sqrt(1/( (1/tau2)+(n/sigma2)))

# No conjugate Prior
library(rstan)
#launch Stan model
data<- list(N=n, y=y, sigma =sqrt(sigma2), mu = mu, tau = sqrt(tau2))
fit <- stan(file="files/normal.stan",data = data, chains = 4, iter=2000)

#extract Stan output
sim <- extract(fit)

quantile(sim$theta)
boxplot(sim$theta, horizontal = TRUE, col = "lightgray", main = "Theta Box Plot")
```

from the numerical value of the quantiles it seems the theta distribution is well distribuited around the mean, infact the quantiles are at the same distance from it. Moreover it is immediately clear by looking on the box plot: 
the various theta values are, more or less, simmetrically spreaded around the mean.

```{r basic = 8, echo=TRUE}
# plot of the simulated posterior and the true posterior
hist(sim$theta, probability = TRUE, main = "Theta's Histogram", xlab = "theta")
curve(dnorm(x, mu_star, sd_star), 
     xlab=expression(theta), ylab="", col="blue", lwd=2,
     cex.lab=2, add=T)
```

In above plot we observed that normal curve and histogram of theta parameter coincide and just to show it in different perspective we used qqplot.

```{r basic=9, echo=TRUE}
qqnorm(sim$theta, pch = 1, frame = FALSE, main = "Normal Q-Q Plot for theta")
qqline(sim$theta, col = "steelblue", lwd = 2)

```
the Q-Q plot shows us what suppose until know, the theta  is distributed like a normal distribution, even if a there is a tiny variability on the tails.

### Exercise 6

**Launch the following line of $R$ code:**
```{r basic= 10, echo=TRUE}
posterior <- as.array(fit)
```

**Use now the $bayesplot$ package. Read the help and produce for this example, using the object posterior, the following plots:**

* **posterior intervals.**
* **posterior areas.**
* **marginal posterior distributions for the parameters.**

**Quickly comment.**

**Solution:**

First, we print the dimensions of the posterior object : 
```{r basic= 10, echo=TRUE}
dim(posterior)
dimnames(posterior)
```

**1. Plot posterior intervals.**

Central posterior uncertainty intervals can be plotted using the $mcmc_intervals$ from $bayesplot$ package. 

```{r basic=10.1 , echo=TRUE,  message=FALSE, warning=FALSE}
library(bayesplot)
mcmc_intervals(posterior, pars = c("theta"))

```

The point in the above plot is the posterior medians.Also, the default is to show 50% intervals (the thick segments) and 90% intervals (the thinner outer lines). 

**2. Plot posterior areas.**

```{r basic=10.2 , echo=TRUE,  message=FALSE, warning=FALSE}
plot_title <- ggtitle("Posterior distributions",
                      "with medians and 80% intervals")

mcmc_areas(posterior, 
           pars = "theta", 
           prob = 0.8, # 80% intervals
           prob_outer = 0.99, # 99%
           point_est = "median") + plot_title
```

This plot shows the estimated posterior density curve and under the curve is the uncertainty intervals as shaded area. 

*3. marginal posterior distributions for the parameters.*

```{r basic=10.3 , echo=TRUE,  message=FALSE, warning=FALSE}
#histogram of theta parameter
mcmc_hist(posterior, pars = c("theta"))

```

Here, `mcmc_hist` plots the marginal posterior distributions with all Markov Chains combined. 


### Exercise 7

**Suppose you receive $n=15$ phone calls in a day, and you want to build a model to assess their average length. Your likelihood for each call length is $yi∼Poisson(\lambda)$. Now, you have to choose the prior $π(λ)$. Please, tell which of these priors is adequate to describe the problem, and provide a short motivation for each of them:**

1. $π(\lambda)=Beta(4,2);$

2. $π(\lambda)=Normal(1,2);$

3. $π(\lambda)=Gamma(4,2);$

**Now, compute your posterior as $\pi(\lambda|y)∝L(\lambda;y)\pi(\lambda)$ for the selected prior. If your first choice was correct, you will be able to compute it analitically.**

**Solution:**

Remining that a prior distribution is our prior belief on the $\lambda$ paramenter, we may observe the following notes:

1. The Beta distribution is a continuous distribution but his support is defined on the range $[0, 1]$, and means to consider the everage time in days or more that is not realistic.So, it is not convinient to use it because $\lambda$ belongs to the range $[0, +\infty)$. Moreover, the Beta distribution is usually associated with the Bernoulli distribution.

2. The Normal distribution is a continuous distribution and it considers a larger set of values in respect to the Beta distribution but, also in this case, it is not a suitable prior distribution for $\lambda$ because it is defined on the range $(-\infty, +\infty)$, so it allows negative values that are not acceptable for $\lambda$.

3. The Gamma distribution is a continuous distribution defined on the range $[0, +\infty)$, so it is well defined for the $\lambda$ parameter. Moreover, if the likelihood is rapresented by a Poisson distribution (like in this case), the Gamma distribution is not just a prior distribution but it is a conjugate prior. It means the posterior and the prior distributions belong to the same probability distribution family and this is great because we are able to analitically compute the posterior distribution.

As a final thought, we may also consider times and computational costs: the gamma as posterior distribution is a well known distribution while the posteriors given by the others priors force to use the MCMC algorithm and it might be expensive and gives an approximately solution.

We may consider a general case in which we have a Poisson model $X_1, X_2, \dots, X_n$ ~ $Poisson(\lambda)$ where $X_1, X_2, \dots, X_n$ are iid. While as a prior distribution suppose to have a $Gamma(\alpha, \beta)$. Then, the posterior distribution is defined as follows:

$p(\lambda | X) \propto p(X | \lambda) \cdot p(\lambda)$
$\propto \prod_{i = 1}^{n}(\frac{\lambda^{x_i} \cdot e^{-\lambda}}{x_i !}) \cdot \frac{\beta^\alpha}{\Gamma(\alpha)} \cdot \lambda^{\alpha - 1} \cdot e^{\beta \cdot \lambda}$
$\propto \prod_{i = 1}^{n}(\lambda^{x_i}) \cdot e^{-n \cdot \lambda} \cdot \lambda^{\alpha - 1} \cdot e^{\beta \cdot \lambda} = \lambda^{(s + \alpha) - 1} \cdot e^{-(n + \beta) \cdot \lambda}$

Where we have not compute the normalization constant and we ignored constants that do not depend on $\lambda$.
So, the posterior distribution belongs to a gamma distribution $Gamma(s + \alpha, n + \beta)$ in which $s = x_1 + x_2 + \cdots + x_n$; in the our specific case the posterior distribution belongs to a $Gamma(s + 4, 15 + 2)$.



### Exercise 8

**Go to this link: [rstan](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started), and follow the instructions to download and install the `rstan` library. Once you did it succesfully, open the file model called `biparametric.stan`, and replace the line:**

`target+ = cauchy_lpdf(sigma | 0, 2.5);`

**with the following one:**

`target+ = uniform_lpdf(sigma | 0.1, 10);`

**Which prior are you now assuming for your parameter**$\sigma$**? Reproduce the same plots as above and briefly comment. **

**Solution:**

We are assuming uniform distribution for parameter $\sigma$ such that $\sigma \sim \text{Unif}(0.1,10)$.

```{r basic= 17,echo=TRUE}
library("bayesplot")
library("rstan")
library("ggplot2")

theta_sample <- 2
sigma2 <- 2
n <- 10
y <- rnorm(n,theta_sample, sqrt(sigma2))

data <- list(N=n, y=y, a=-10, b=10)
fit <- stan(file ="files/biparametric.stan", data = data , chains = 4, iter=2000, refresh=-1)
sim <- extract(fit)
posterior_biv <- as.matrix(fit)

theta_est <- mean(sim$theta)
theta_var <- var(sim$theta)
sigma_est <- mean(sim$sigma)
sigma_var <- var(sim$sigma)
c(theta_est,theta_var, sigma_est,sigma_var)
traceplot(fit, pars=c("theta", "sigma"))

plot_title <- ggtitle("Posterior distributions",
                      "with medians and 80% intervals")

mcmc_areas(posterior_biv, 
           pars = c("theta","sigma"), 
           prob = 0.8) + plot_title

```

From the traceplot, we can easily say that chains are converged meaning they run around some values of the parameter space. There is no significant difference between old and new posterior distribution plots so we may say that prior distributions don't provide much information about posterior distributions.


### Exercise 9

**Reproduce the first plot above for the soccer goals, but this time by replacing Prior 1 with a **$\text{Gamma}(2,4).$**Then, compute the final Bayes factor matrix `(BF_matrix)` with this new prior and the other ones unchanged, and comment. Is still Prior 2 favorable over all the others? **

**Solution:**

Likelihood and prior distributions are given in labratory file. Prior one changed from $\text{Gamma}(4.57,1.43)$ to $\text{Gamma}(2,4)$ (average $\alpha/\beta = 0.5$) and first plot reproduced below. 

```{r basic= 18,echo=TRUE,  message=FALSE, warning=FALSE}
library(LearnBayes)
data(soccergoals)

y <- soccergoals$goals

#write the likelihood function via the gamma distribution


lik_pois<- function(data, theta){
  n <- length(data)
  lambda <- exp(theta)
  dgamma(lambda, shape =sum(data)+1, scale=1/n)
}

 prior_gamma <- function(par, theta){
  lambda <- exp(theta)
  dgamma(lambda, par[1], rate=par[2])*lambda  
}

 prior_norm <- function(npar, theta){
 lambda=exp(theta)  
 (dnorm(theta, npar[1], npar[2]))
  
}

lik_pois_v <- Vectorize(lik_pois, "theta")
prior_gamma_v <- Vectorize(prior_gamma, "theta")
prior_norm_v <- Vectorize(prior_norm, "theta")


#likelihood
 curve(lik_pois_v(theta=x, data=y), xlim=c(-1,4), xlab=expression(theta), ylab = "density", lwd =2 )
#prior 1
 curve(prior_gamma_v(theta=x, par=c(2, 4)), lty =2, col="red", add = TRUE, lwd =2)
#prior 2 
 curve(prior_norm_v(theta=x, npar=c(1, .5)), lty =3, col="blue", add =TRUE, lwd=2)
#prior 3 
 curve(prior_norm_v(theta=x, npar=c(2, .5)), lty =4, col="green", add =TRUE, lwd =2)
#prior 4 
  curve(prior_norm_v(theta=x, npar=c(1, 2)), lty =5, col="violet", add =TRUE, lwd =2)
  legend(2.6, 1.8, c("Lik.", "Prior 1: Ga(2,4)", "Prior 2: N(1, 0.25)", "Prior 3: N(2,0.25)","Prior 4: N(1, 4)" ),
  lty=c(1,2,3,4,5), col=c("black", "red", "blue", "green", "violet"),lwd=2, cex=0.9)
```

In previous plot, `Prior 1` and `Prior 2` coincided but now they are different because of the change we made. Expected value of likelihood centered around $0.5$ and each prior offered different estimation and variance. `Prior 3` is concentrated around 2 which seems higher than other priors. `Prior 4` shows more spreaded distribution so it is the less informative one among the others. 

```{r basic= 19,echo=TRUE,  message=FALSE, warning=FALSE}
logpoissongamma <- function(theta, datapar){
   data <- datapar$data
   par <- datapar$par
   lambda <- exp(theta)
   log_lik <- log(lik_pois(data, theta))
   log_prior <- log(prior_gamma(par, theta))
   return(log_lik+log_prior)
}

logpoissongamma.v <- Vectorize( logpoissongamma, "theta")


logpoissonnormal <- function( theta, datapar){
 data <- datapar$data
 npar <- datapar$par
 lambda <- exp(theta)
 log_lik <- log(lik_pois(data, theta))
 log_prior <- log(prior_norm(npar, theta))
  return(log_lik+log_prior)
}  
logpoissonnormal.v <- Vectorize( logpoissonnormal, "theta")

#log-likelihood
curve(log(lik_pois(y, theta=x)), xlim=c(-1,4),ylim=c(-20,2), lty =1,
   ylab="log-posteriors", xlab=expression(theta))
#log posterior 1
curve(logpoissongamma.v(theta=x, list(data=y, par=c(2, 4))), col="red", xlim=c(-1,4),ylim=c(-20,2), lty =1, add =TRUE)
#log posterior 2
 curve(logpoissonnormal.v( theta=x, datapar <- list(data=y, par=c(1, .5))), lty =1, col="blue",  add =TRUE)
#log posterior 3
 curve(logpoissonnormal.v( theta=x, datapar <- list(data=y, par=c(2, .5))), lty =1, col="green", add =TRUE, lwd =2)
#log posterior 4
  curve(logpoissonnormal.v( theta=x, list(data=y, par=c(1, 2))), lty =1, col="violet", add =TRUE, lwd =2)
 legend(2.6, 1.3, c( "loglik", "lpost 1", "lpost 2", "lpost 3", "lpost 4" ),
  lty=1, col=c("black", "red", "blue", "green", "violet"),lwd=2, cex=0.9)
```

Curves of posterior distributions are quite similar among the each other. They are close to he log-likeligood function. The only curve stands out is the one more in constrast with likelihood. Comparasions of these models will be made by using bayes factor.

```{r basic= 20, echo=TRUE,  message=FALSE, warning=FALSE}
datapar <- list(data=y, par=c(2, 4))
fit1 <- laplace(logpoissongamma, .5, datapar)
datapar <- list(data=y, par=c(1, .5))
fit2 <- laplace(logpoissonnormal, .5, datapar)
datapar <- list(data=y, par=c(2, .5))
fit3 <- laplace(logpoissonnormal, .5, datapar)
datapar <- list(data=y, par=c(1, 2))
fit4 <- laplace(logpoissonnormal, .5, datapar)

postmode <- c(fit1$mode, fit2$mode, fit3$mode, fit4$mode )
postsds <- sqrt(c(fit1$var, fit2$var, fit3$var, fit4$var))
logmarg <- c(fit1$int, fit2$int, fit3$int, fit4$int)
cbind(postmode, postsds, logmarg)
```

Here we found the posterior mode, log marginal likelihood and posterior standard deviations. Posterior mode of `Prior 1` is bit smaller than others. It is due to influence of the prior distribution concentrated around smaller values compared the likelihood.

```{r basic= 21, echo=TRUE,  message=FALSE, warning=FALSE}
BF_matrix <- matrix(1, 4,4)
for (i in 1:3){
  for (j in 2:4){
   BF_matrix[i,j]<- exp(logmarg[i]-logmarg[j])
   BF_matrix[j,i]=(1/BF_matrix[i,j]) 
  }
}

round_bf <- round(BF_matrix,3)
round_bf
```

`Prior 2` is still favored over other priors. The change that we did to `Prior 1` made negative impact and it perform worse than previous prior choice and still every prior is favored over `Prior 3`.


### Exercise 10

**Let $y=(1,0,0,1,0,0,0,0,0,1,0,0,1,0)$ collect the results of tossing $n=14$ times an unfair coin, where 1 denotes _heads_ and 0 _tails_, and $p=\text{Prob}(y_i=1)$.**

1. **Looking at the `Stan` code for the other models, write a short Stan Beta-Binomial model, where $p$ has a $\text{Beta}(a,b)$ prior with $a=3, b=3$.**

```
The stan file used is:
data{
  int N;
  int y;
  real<lower=0> alpha;
  real<lower=0> beta;
}

parameters{
  real theta;
}

model{
  target+=binomial_lpmf(y|N, theta); 
  target+=beta_lpdf(theta|alpha, beta);
}
```

* **extract the posterior distribution with the function `extract()`**

```{r basic= 22, echo=TRUE,  message=FALSE, warning=FALSE}

y=c(1,0,0,1,0,0,0,0,0,1,0,0,1,0)
n <-  length(y)
heads <- sum(y == 1)
tails <- n - heads

alpha = 3
beta = 3

data<- list(N=n, y=heads, alpha=alpha, beta=beta)
fit <- stan(file="files/beta-binomial.stan", data = data, chains = 4, iter=2000,refresh=-1)
#extract the Stan output
sim <- extract(fit)

```

* **produce some plots with the `bayesplot` package and comment.**

```{r basic= 23, echo=TRUE,  message=FALSE, warning=FALSE}

library(LearnBayes)
library(bayesplot)

summary(sim$theta)
var(sim$theta)

posterior <- as.matrix(fit)

#traceplot
traceplot(fit, pars ="theta")

mcmc_intervals(posterior, pars = c("theta"))

plot_title <- ggtitle("Posterior distributions","with medians and 80% intervals")

mcmc_areas(posterior, pars = c("theta"), prob = 0.8) + plot_title
```

It can be observed that chains are converged, in other words they run around some values. Estimation is $0.35$ with variance $0.011$ which isn't very large. Second plot shows estimation median which is around $0.35$ with dark blue line and blue shaded area is the 80% confidence interval of median.

* **compute analitically the posterior distribution and compare it with the `Stan` distribution.**

Likelihood is binomial distribution with the following parameters: (H denotes heads, T denotes tails)
$$
\text{Bin}(H|\theta,H+T)
$$
Prior is beta distribution with the following parameters:
$$
\text{Beta}(\theta|\alpha,\beta)
$$
Beta distribution is conjugate prior of binomial distribution so:
$Beta(\alpha,\beta)$ prior and $x\sim Bin(n,\pi)$ likelihood result in the posterior

\begin{align}
p(\pi|x,\alpha,\beta)&\propto \pi^x (1-\pi)^{n-x} \pi^{\alpha-1}(1-\pi)^{\beta-1}\\
                     &\propto \pi^{x+\alpha-1}(1-\pi)^{n-x+\beta-1}
\end{align}

which is a $Beta(x+\alpha, n-x+\beta)$.

So likelihood is:
$$
p(\theta|H+T) \propto \text{Bin}(H|\theta,H+T) \times \text{Beta}(\theta|\alpha,\beta) \propto \text{Beta}(\theta|\alpha + H, \beta + T)
$$
Since we know that $H = 4$, $T=10$ and $\alpha=3$, $\beta=3$, we have $\text{Beta}(7,13)$

```{r echo=TRUE,  message=FALSE, warning=FALSE}
curve(dbeta(x,alpha,beta), xlim=c(-0.25,1.25), ylim= c(0,4), lwd =2,ylab = "Density", xlab = expression(theta))
curve(dbeta(x,alpha+4,beta+10), lty =2, col="red", add=TRUE,lwd=2)
lines(density(sim$theta, adj=2),lty=2, col="blue", add=TRUE)
legend(0.8,3.5,c("Prior", "True Posterior","Stan Posterior"), c("black", "red","blue"))

```
As we can understand from the plot, true posterior and stand posterior quite close to each other.
